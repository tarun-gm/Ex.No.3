
---

# EXPERIMENT – 3: Scenario-Based Report Development Using Diverse Prompting Approaches

### Register No.: 2122232301XX

### Date: 05.09.2025

---

## Aim:

To develop an AI-driven customer support assistant that can effectively address **troubleshooting requests, order-related queries, and general FAQs**. The chatbot should maintain a natural, conversational style and handle multiple user contexts. In this experiment, **Straightforward Prompts, Tabular Format Prompting, and Preceding Question Prompting** are applied to demonstrate their impact on chatbot responses.

---

## Apparatus/Tools Required:

* AI text generation system (e.g., ChatGPT, Gemini, Claude)
* Spreadsheet/Document software for organizing prompts and results
* Internet connectivity for testing interactions

---

## Theory:

Prompt engineering involves formulating queries in a structured way to guide AI models towards accurate and user-friendly outputs. Different prompting strategies can drastically change the effectiveness of chatbot responses.

1. **Straightforward Prompting**

   * Direct, concise commands or questions.
   * Best suited for short answers and fact-based queries.

2. **Tabular Format Prompting**

   * Asks the model to present results in structured tables.
   * Helps in displaying step-by-step guides, comparisons, or multi-item information.

3. **Preceding Question Prompting**

   * Builds on the previous query, creating continuity.
   * Useful for **human-like, context-aware conversations**.

By applying these prompting methods, the chatbot becomes adaptable for varied customer support scenarios.

---

## Experiment Procedure:

We tested **three typical customer queries** using all three prompting techniques. Each interaction was recorded and analyzed.

---

### Scenario 1: Product Troubleshooting

* **Straightforward Prompt Example:**
  *“How do I fix if my smart vacuum cleaner is not charging?”*

* **Tabular Prompt Example:**
  *“List possible causes and fixes for a vacuum cleaner that won’t charge in a table.”*

* **Preceding Question Prompt Example:**
  *Q1: “My vacuum isn’t charging.”*
  *Q2: “It shows a red blinking light, what does that mean?”*

---

### Scenario 2: Order Tracking

* **Straightforward Prompt Example:**
  *“Where is my order #24567 placed on 1st September?”*

* **Tabular Prompt Example:**
  *“Show the shipping stages of my order in table format (Ordered, Packed, Shipped, Out for delivery).”*

* **Preceding Question Prompt Example:**
  *Q1: “Track order #24567.”*
  *Q2: “It says ‘in transit.’ Can you tell me when it will arrive?”*

---

### Scenario 3: General Inquiry

* **Straightforward Prompt Example:**
  *“What is your customer service helpline number?”*

* **Tabular Prompt Example:**
  *“Provide a table listing working hours, helpline numbers, and email IDs for support.”*

* **Preceding Question Prompt Example:**
  *Q1: “Do you have weekend support?”*
  *Q2: “Okay, then how can I contact you on Sunday?”*

---

## Observations & Evaluation:

* **Straightforward Prompting** → Best for quick queries like contact numbers or order IDs.
* **Tabular Prompting** → Extremely useful for troubleshooting steps or multiple options, as it avoids confusion.
* **Preceding Question Prompting** → Gave the most natural, continuous conversation flow. Customers feel like they are chatting with a real agent.

---

## Conclusion:

By employing **Straightforward, Tabular, and Preceding Question Prompting**, the chatbot could handle a variety of customer needs smoothly. Each prompting style enhanced a different aspect of interaction — speed, clarity, or conversational flow — making the chatbot versatile and customer-friendly.

---

## Result:

Thus, the experiment was successfully carried out, and the chatbot responses improved significantly with diverse prompting strategies. The system demonstrated the ability to assist customers in **troubleshooting, tracking orders, and handling general inquiries** effectively.

---
